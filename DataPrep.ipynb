{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_csv_path = 'Datasets/combined_dataset.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label                                               text\n",
      "0      1  Se não querem o Varandas como Presidente caso ...\n",
      "1      0  Le produit de Sony ne semble pas presque aussi...\n",
      "2      1  Se alguém falar mal de ti pelas costas ..... P...\n",
      "3      0                     @HatanoSayuri Me obriga &gt;:(\n",
      "4      1                 @mana_eliana Oi? Estou à espera :)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(input_csv_path)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label    0\n",
      "text     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_phase1(text):\n",
    "    \"\"\"\n",
    "    Initial text cleaning: lowercase, remove URLs, mentions, special characters, digits.\n",
    "    \"\"\"\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/chyavanshenoy/nltk_data/corpora/stopwords\n",
      "/Users/chyavanshenoy/nltk_data/tokenizers/punkt\n"
     ]
    }
   ],
   "source": [
    "print(nltk.data.find('corpora/stopwords'))\n",
    "print(nltk.data.find('tokenizers/punkt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_WORDS_ENGLISH = set()\n",
    "STOP_WORDS_PORTUGUESE = set()\n",
    "STOP_WORDS_FRENCH = set()\n",
    "STOP_WORDS_NEPALI = set()\n",
    "STOP_WORDS_HINDI = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_WORDS_ENGLISH = set(stopwords.words('english'))\n",
    "STOP_WORDS_PORTUGUESE = set(stopwords.words('portuguese'))\n",
    "STOP_WORDS_FRENCH = set(stopwords.words('french'))\n",
    "STOP_WORDS_NEPALI = set(stopwords.words('nepali'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMBINED_STOPWORDS = STOP_WORDS_ENGLISH.union(STOP_WORDS_PORTUGUESE) \\\n",
    "                                 .union(STOP_WORDS_FRENCH) \\\n",
    "                                 .union(STOP_WORDS_NEPALI) \\\n",
    "                                 .union(STOP_WORDS_HINDI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords_from_tokens(text_cleaned_phase1):\n",
    "    \"\"\"\n",
    "    Tokenizes text and removes stopwords and non-alphabetic tokens.\n",
    "    Assumes text is already lowercased and had URLs, mentions, etc. removed.\n",
    "    \"\"\"\n",
    "    if not text_cleaned_phase1 or pd.isna(text_cleaned_phase1):\n",
    "        return \"\"\n",
    "        \n",
    "    tokens = word_tokenize(text_cleaned_phase1)\n",
    "    \n",
    "    filtered_tokens = [\n",
    "        word for word in tokens if word.isalpha() and word not in COMBINED_STOPWORDS\n",
    "    ]\n",
    "    return ' '.join(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_intermediate_clean'] = df['text'].astype(str).apply(clean_text_phase1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  Se não querem o Varandas como Presidente caso ...   \n",
      "1  Le produit de Sony ne semble pas presque aussi...   \n",
      "2  Se alguém falar mal de ti pelas costas ..... P...   \n",
      "3                     @HatanoSayuri Me obriga &gt;:(   \n",
      "4                 @mana_eliana Oi? Estou à espera :)   \n",
      "\n",
      "                             text_intermediate_clean  \n",
      "0  se não querem o varandas como presidente caso ...  \n",
      "1  le produit de sony ne semble pas presque aussi...  \n",
      "2       se alguém falar mal de ti pelas costas peide  \n",
      "3                                       me obriga gt  \n",
      "4                                  oi estou à espera  \n"
     ]
    }
   ],
   "source": [
    "print(df[['text', 'text_intermediate_clean']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_clean'] = df['text_intermediate_clean'].apply(remove_stopwords_from_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             text_intermediate_clean  \\\n",
      "0  se não querem o varandas como presidente caso ...   \n",
      "1  le produit de sony ne semble pas presque aussi...   \n",
      "2       se alguém falar mal de ti pelas costas peide   \n",
      "3                                       me obriga gt   \n",
      "4                                  oi estou à espera   \n",
      "\n",
      "                                          text_clean  \n",
      "0  querem varandas presidente caso vença eleições...  \n",
      "1  produit sony semble presque aussi intéressant ...  \n",
      "2                   alguém falar mal ti costas peide  \n",
      "3                                          obriga gt  \n",
      "4                                          oi espera  \n"
     ]
    }
   ],
   "source": [
    "print(df[['text_intermediate_clean', 'text_clean']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_cleaned = df[['text_clean', 'label']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_cleaned.dropna(subset=['text_clean', 'label'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_cleaned = df_final_cleaned[df_final_cleaned['text_clean'].str.strip().astype(bool)]\n",
    "df_final_cleaned.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    0.483724\n",
      "1    0.449766\n",
      "2    0.066510\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(df_final_cleaned['label'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory = 'Datasets'\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_path = os.path.join(output_directory, 'combined_dataset.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_cleaned.to_parquet(output_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
