{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TheOctoMizer/AAI-510-Project/blob/main/FinalProjectSentimentAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentimenet Analysis using multilingual texts:\n"
      ],
      "metadata": {
        "id": "6vZHEnGDczoe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Business Understanding\n",
        "\n",
        "Twitter sentiment analysis helps brands, governments, and researchers understand public opinion in real time.\n",
        "\n",
        "**Business Need:** Build a multilingual sentiment classifier (English, French, Portuguese) that classifies tweets as:\n",
        "- Positive\n",
        "- Negative\n",
        "\n",
        "This will enable:\n",
        "- Monitoring brand perception\n",
        "- Tracking political sentiment\n",
        "- Analyzing feedback across diverse markets"
      ],
      "metadata": {
        "id": "wV21yqfyev9e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Data Understanding\n",
        "\n",
        "You have 3 datasets:\n",
        "- ðŸ‡¬ðŸ‡§ English: 100k+ samples with text and sentiment\n",
        "- ðŸ‡«ðŸ‡· French: ~9 lakh samples, but lacks \"neutral\"\n",
        "- ðŸ‡µðŸ‡¹ Portuguese: ~6 lakh samples\n",
        "\n",
        "Challenges:\n",
        "- Label format inconsistencies (e.g., 0/1/2, strings)\n",
        "- Extra columns\n",
        "- Missing/imbalanced classes"
      ],
      "metadata": {
        "id": "dioyY7e0f30j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from datasets import Dataset\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score"
      ],
      "metadata": {
        "id": "tA_kX070h-d-"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Data Preparation\n",
        "\n",
        "Steps:\n",
        "- Clean column formats\n",
        "- Drop extra columns\n",
        "- Map labels to 'positive'/'negative'\n",
        "- Remove neutral samples\n",
        "- Stratified downsample to 100k per language\n",
        "- Combine into 300k multilingual dataset"
      ],
      "metadata": {
        "id": "oklRi1Qvh6_V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "base_path = '/content/drive/My Drive/AAI-510-Dataset'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-xiXsTHiYIc",
        "outputId": "3dd7b6d3-20a0-4c5f-89de-05bb4d6c48a4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def stratified_downsample(df, sample_size):\n",
        "    label_dist = df['label'].value_counts(normalize=True).to_dict()\n",
        "    samples = []\n",
        "    for label, ratio in label_dist.items():\n",
        "        n = int(sample_size * ratio)\n",
        "        part = df[df['label'] == label].sample(n=n, random_state=42)\n",
        "        samples.append(part)\n",
        "    return pd.concat(samples).sample(frac=1, random_state=42).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "RoBGEePUcyUY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# ------------------------\n",
        "# ðŸŸ¢ 1. Portuguese Dataset\n",
        "# ------------------------\n",
        "\n",
        "def process_portuguese(input_path, output_path):\n",
        "    df = pd.read_csv(input_path, sep=';', quoting=3, encoding='utf-8', on_bad_lines='skip')\n",
        "\n",
        "    # Keep only necessary columns\n",
        "    df = df[['tweet_text', 'sentiment']]\n",
        "    df.columns = ['text', 'label']\n",
        "    df['language'] = 'pt'\n",
        "\n",
        "    label_map = {\n",
        "        '0': 'negative', '1': 'positive', '2': 'neutral',\n",
        "        0: 'negative', 1: 'positive', 2: 'neutral'\n",
        "    }\n",
        "    df['label'] = df['label'].map(label_map)\n",
        "    df = df[df['label'].isin(['positive', 'negative'])]\n",
        "    df.info()\n",
        "    # Stratified downsample\n",
        "    sampled = stratified_downsample(df, 65000)\n",
        "    sampled.to_csv(output_path, index=False)\n",
        "    print(f\"âœ… Portuguese dataset saved: {output_path}\")"
      ],
      "metadata": {
        "id": "6OaquI2ehq09"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------\n",
        "# ðŸŸ¢ 2. English Dataset\n",
        "# --------------------\n",
        "\n",
        "def process_english(input_path, output_path):\n",
        "    df = pd.read_csv(input_path)\n",
        "    df = df[['Text', 'Label']]\n",
        "    df.columns = ['text', 'label']\n",
        "    df['language'] = 'en'\n",
        "\n",
        "    df['label'] = df['label'].astype(str).str.lower().str.strip()\n",
        "    df = df[df['label'].isin(['positive', 'negative'])]\n",
        "\n",
        "    sampled = stratified_downsample(df, 65000)\n",
        "    sampled.to_csv(output_path, index=False)\n",
        "    print(f\"âœ… English dataset saved: {output_path}\")"
      ],
      "metadata": {
        "id": "ljh4gox2ht5n"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------\n",
        "# ðŸŸ¢ 3. French Dataset\n",
        "# -------------------\n",
        "\n",
        "def process_french(input_path, output_path):\n",
        "    df = pd.read_csv(input_path)\n",
        "    df = df[['text', 'label']]\n",
        "    df.columns = ['text', 'label']\n",
        "    df['language'] = 'fr'\n",
        "\n",
        "    label_map = {\n",
        "        '0': 'negative', '1': 'positive', '2': 'neutral',\n",
        "        0: 'negative', 1: 'positive', 2: 'neutral'\n",
        "    }\n",
        "    df['label'] = df['label'].map(label_map)\n",
        "    df = df[df['label'].isin(['positive', 'negative'])]\n",
        "\n",
        "    sampled = stratified_downsample(df, 65000)\n",
        "    sampled.to_csv(output_path, index=False)\n",
        "    print(f\"âœ… French dataset saved: {output_path}\")"
      ],
      "metadata": {
        "id": "PgJxNQqdhvuO"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "process_portuguese(f\"{base_path}/portuguese.csv\", f\"{base_path}/portuguese_cleaned_65k.csv\")\n",
        "process_english(f\"{base_path}/english.csv\", f\"{base_path}/english_cleaned_65k.csv\")\n",
        "process_french(f\"{base_path}/french.csv\", f\"{base_path}/french_cleaned_65k.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZAUqKrQhwi0",
        "outputId": "a992feb6-6750-4a05-feb1-e4ed0c486003"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 65470 entries, 0 to 65469\n",
            "Data columns (total 3 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   text      65470 non-null  object\n",
            " 1   label     65470 non-null  object\n",
            " 2   language  65470 non-null  object\n",
            "dtypes: object(3)\n",
            "memory usage: 2.0+ MB\n",
            "âœ… Portuguese dataset saved: /content/drive/My Drive/AAI-510-Dataset/portuguese_cleaned_65k.csv\n",
            "âœ… English dataset saved: /content/drive/My Drive/AAI-510-Dataset/english_cleaned_65k.csv\n",
            "âœ… French dataset saved: /content/drive/My Drive/AAI-510-Dataset/french_cleaned_65k.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "en = pd.read_csv(f\"{base_path}/english_cleaned_65k.csv\")\n",
        "pt = pd.read_csv(f\"{base_path}/portuguese_cleaned_65k.csv\")\n",
        "fr = pd.read_csv(f\"{base_path}/french_cleaned_65k.csv\")\n",
        "\n",
        "combined = pd.concat([en, pt, fr])\n",
        "combined = combined.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "combined.to_csv(f\"{base_path}/multilingual_sentiment_195k.csv\", index=False)\n",
        "\n",
        "print(\"âœ… Combined dataset saved: multilingual_sentiment_195k.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LhkQRsichyeT",
        "outputId": "0af8c487-217e-45aa-e120-925afbe4ee27"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Combined dataset saved: multilingual_sentiment_195k.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "FaB6OJLdcnev"
      }
    }
  ]
}