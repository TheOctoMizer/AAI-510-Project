{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TheOctoMizer/AAI-510-Project/blob/main/final-project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Business Understanding\n",
        "\n",
        "Twitter sentiment analysis helps brands, governments, and researchers understand public opinion in real time.\n",
        "\n",
        "**Business Need:** Build a multilingual sentiment classifier (English, French, Portuguese) that classifies tweets as:\n",
        "- Positive\n",
        "- Negative\n",
        "\n",
        "This will enable:\n",
        "- Monitoring brand perception\n",
        "- Tracking political sentiment\n",
        "- Analyzing feedback across diverse markets"
      ],
      "metadata": {
        "id": "_NQQ1fxScibq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Data Understanding\n",
        "\n",
        "You have 3 datasets:\n",
        "- ðŸ‡¬ðŸ‡§ English: 100k+ samples with text and sentiment\n",
        "- ðŸ‡«ðŸ‡· French: ~9 lakh samples, but lacks \"neutral\"\n",
        "- ðŸ‡µðŸ‡¹ Portuguese: ~6 lakh samples\n",
        "\n",
        "Challenges:\n",
        "- Label format inconsistencies (e.g., 0/1/2, strings)\n",
        "- Extra columns\n",
        "- Missing/imbalanced classes"
      ],
      "metadata": {
        "id": "pwW12qRKrBql"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from datasets import Dataset\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score"
      ],
      "metadata": {
        "id": "YwYhjkrLsNLO"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Data Preparation\n",
        "\n",
        "Steps:\n",
        "- Clean column formats\n",
        "- Drop extra columns\n",
        "- Map labels to 'positive'/'negative'\n",
        "- Remove neutral samples\n",
        "- Stratified downsample to 65k per language\n",
        "- Combine into 195k multilingual dataset"
      ],
      "metadata": {
        "id": "5dy8m54QshPv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "base_path = '/content/drive/My Drive/AAI-510-Dataset'"
      ],
      "metadata": {
        "id": "NFsn3rKusYjG",
        "outputId": "49e98b57-9bea-4e1a-80b4-d8f0ebe6438e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def stratified_downsample(df, sample_size):\n",
        "    label_dist = df['label'].value_counts(normalize=True).to_dict()\n",
        "    samples = []\n",
        "    for label, ratio in label_dist.items():\n",
        "        n = int(sample_size * ratio)\n",
        "        part = df[df['label'] == label].sample(n=n, random_state=42)\n",
        "        samples.append(part)\n",
        "    return pd.concat(samples).sample(frac=1, random_state=42).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "fDYQvvFMsqxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# ------------------------\n",
        "# ðŸŸ¢ 1. Portuguese Dataset\n",
        "# ------------------------\n",
        "\n",
        "def process_portuguese(input_path, output_path):\n",
        "    df = pd.read_csv(input_path, sep=';', quoting=3, encoding='utf-8', on_bad_lines='skip')\n",
        "\n",
        "    # Keep only necessary columns\n",
        "    df = df[['tweet_text', 'sentiment']]\n",
        "    df.columns = ['text', 'label']\n",
        "    df['language'] = 'pt'\n",
        "\n",
        "    label_map = {\n",
        "        '0': 'negative', '1': 'positive', '2': 'neutral',\n",
        "        0: 'negative', 1: 'positive', 2: 'neutral'\n",
        "    }\n",
        "    df['label'] = df['label'].map(label_map)\n",
        "    df = df[df['label'].isin(['positive', 'negative'])]\n",
        "    df.info()\n",
        "    # Stratified downsample\n",
        "    sampled = stratified_downsample(df, 65000)\n",
        "    sampled.to_csv(output_path, index=False)\n",
        "    print(f\"âœ… Portuguese dataset saved: {output_path}\")"
      ],
      "metadata": {
        "id": "8WMDdlbwssqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------\n",
        "# ðŸŸ¢ 2. English Dataset\n",
        "# --------------------\n",
        "\n",
        "def process_english(input_path, output_path):\n",
        "    df = pd.read_csv(input_path)\n",
        "    df = df[['Text', 'Label']]\n",
        "    df.columns = ['text', 'label']\n",
        "    df['language'] = 'en'\n",
        "\n",
        "    df['label'] = df['label'].astype(str).str.lower().str.strip()\n",
        "    df = df[df['label'].isin(['positive', 'negative'])]\n",
        "\n",
        "    sampled = stratified_downsample(df, 65000)\n",
        "    sampled.to_csv(output_path, index=False)\n",
        "    print(f\"âœ… English dataset saved: {output_path}\")"
      ],
      "metadata": {
        "id": "ITHE_ZL9susJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------\n",
        "# ðŸŸ¢ 3. French Dataset\n",
        "# -------------------\n",
        "\n",
        "def process_french(input_path, output_path):\n",
        "    df = pd.read_csv(input_path)\n",
        "    df = df[['text', 'label']]\n",
        "    df.columns = ['text', 'label']\n",
        "    df['language'] = 'fr'\n",
        "\n",
        "    label_map = {\n",
        "        '0': 'negative', '1': 'positive', '2': 'neutral',\n",
        "        0: 'negative', 1: 'positive', 2: 'neutral'\n",
        "    }\n",
        "    df['label'] = df['label'].map(label_map)\n",
        "    df = df[df['label'].isin(['positive', 'negative'])]\n",
        "\n",
        "    sampled = stratified_downsample(df, 65000)\n",
        "    sampled.to_csv(output_path, index=False)\n",
        "    print(f\"âœ… French dataset saved: {output_path}\")"
      ],
      "metadata": {
        "id": "OQp_ECsjswRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "process_portuguese(f\"{base_path}/portuguese.csv\", f\"{base_path}/portuguese_cleaned_65k.csv\")\n",
        "process_english(f\"{base_path}/english.csv\", f\"{base_path}/english_cleaned_65k.csv\")\n",
        "process_french(f\"{base_path}/french.csv\", f\"{base_path}/french_cleaned_65k.csv\")"
      ],
      "metadata": {
        "id": "x7_fhiPLsx_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "en = pd.read_csv(f\"{base_path}/english_cleaned_65k.csv\")\n",
        "pt = pd.read_csv(f\"{base_path}/portuguese_cleaned_65k.csv\")\n",
        "fr = pd.read_csv(f\"{base_path}/french_cleaned_65k.csv\")\n",
        "\n",
        "df_all = pd.concat([en, pt, fr])\n",
        "df_all = df_all.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "df_all.to_csv(f\"{base_path}/multilingual_sentiment_195k.csv\", index=False)\n",
        "\n",
        "print(\"âœ… Combined dataset saved: multilingual_sentiment_195k.csv\")"
      ],
      "metadata": {
        "id": "OZZ5DLU5s0Eb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode labels\n",
        "df_all['label_enc'] = df_all['label'].map({'negative': 0, 'positive': 1})"
      ],
      "metadata": {
        "id": "Dkwyr8Hfs1uZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Exploratory Data Analysis"
      ],
      "metadata": {
        "id": "j3LTn083s5hY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(data=df_all, x='label', hue='language')\n",
        "plt.title(\"Sentiment Distribution by Language\")\n",
        "plt.show()\n",
        "\n",
        "print(df_all['label'].value_counts())"
      ],
      "metadata": {
        "id": "o4xsSepRs3O_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Modeling\n",
        "\n",
        "Models to train:\n",
        "1. XLM-RoBERTa Base\n",
        "2. MDeBERTa v3 Base\n",
        "3. DistilBERT Multilingual\n",
        "4. LSTM\n",
        "\n",
        "Each model is trained and evaluated on the same train/test split."
      ],
      "metadata": {
        "id": "Xzq_7jKOs8Nv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login()"
      ],
      "metadata": {
        "id": "IL_9VHios8uR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import numpy as np\n",
        "\n",
        "def train_transformer_model(model_name, train_ds, test_ds, test_df):\n",
        "    # Load tokenizer and model\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "\n",
        "    # Tokenization function (handles NaNs and ensures text format)\n",
        "    def tokenize(batch):\n",
        "        # Ensure all entries are strings\n",
        "        texts = [str(t) if t is not None else \"\" for t in batch[\"text\"]]\n",
        "        return tokenizer(texts, truncation=True, padding=\"max_length\", max_length=128)\n",
        "\n",
        "    # Apply tokenization\n",
        "    train_encoded = train_ds.map(tokenize, batched=True)\n",
        "    test_encoded = test_ds.map(tokenize, batched=True)\n",
        "\n",
        "    # Define training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f\"./results_{model_name.replace('/', '_')}\",\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=32,\n",
        "        num_train_epochs=2,\n",
        "        save_strategy=\"no\",\n",
        "        logging_dir='./logs',\n",
        "        logging_steps=100,\n",
        "        load_best_model_at_end=False\n",
        "    )\n",
        "\n",
        "    # Define metric computation\n",
        "    def compute_metrics(pred):\n",
        "        labels = pred.label_ids\n",
        "        preds = np.argmax(pred.predictions, axis=1)\n",
        "        return {\n",
        "            \"accuracy\": accuracy_score(labels, preds),\n",
        "            \"f1\": f1_score(labels, preds)\n",
        "        }\n",
        "\n",
        "    # Initialize Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_encoded,\n",
        "        eval_dataset=test_encoded,\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    trainer.train()\n",
        "\n",
        "    # Predict on test set\n",
        "    predictions = trainer.predict(test_encoded)\n",
        "    y_pred = np.argmax(predictions.predictions, axis=1)\n",
        "    y_true = test_df['label_enc'].values\n",
        "\n",
        "    return y_pred, y_true"
      ],
      "metadata": {
        "id": "Z4u8qW2DtARJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### XLM-RoBERTa Base Model training"
      ],
      "metadata": {
        "id": "psMJCs_KtHF2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming df is your full multilingual dataframe\n",
        "train_df, test_df = train_test_split(df_all, test_size=0.2, stratify=df_all['label_enc'], random_state=42)\n",
        "\n",
        "# Convert to Hugging Face Datasets\n",
        "train_ds = Dataset.from_pandas(train_df)\n",
        "test_ds = Dataset.from_pandas(test_df)\n",
        "\n",
        "# Train and evaluate\n",
        "y_pred, y_true = train_transformer_model(\"xlm-roberta-base\", train_ds, test_ds, test_df)"
      ],
      "metadata": {
        "id": "7DBKt_zVtDvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Evaluation"
      ],
      "metadata": {
        "id": "6KIEUtjDtK_0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Final Conclusion\n",
        "\n",
        "| Model             | Accuracy | F1-Score |\n",
        "|------------------|----------|----------|\n",
        "| MDeBERTa v3 Base  | XX.XX%   | XX.XX%   |\n",
        "| XLM-RoBERTa Base  | XX.XX%   | XX.XX%   |\n",
        "| DistilBERT Multi  | XX.XX%   | XX.XX%   |\n",
        "| LSTM              | XX.XX%   | XX.XX%   |\n",
        "\n",
        "**Insights:**\n",
        "- MDeBERTa v3 and XLM-RoBERTa gave best multilingual performance.\n",
        "- DistilBERT is lighter but less accurate.\n",
        "- LSTM works but lags behind modern transformers.\n",
        "\n",
        "**Next Steps:**\n",
        "- Try adding attention to LSTM\n",
        "- Use more data (with neutral)\n",
        "- Test on other languages (e.g., Spanish, Hindi)"
      ],
      "metadata": {
        "id": "_tY030pctMgu"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}