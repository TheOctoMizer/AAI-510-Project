{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_NQQ1fxScibq"
   },
   "source": [
    "## 1. Business Understanding\n",
    "\n",
    "Twitter sentiment analysis helps brands, governments, and researchers understand public opinion in real time.\n",
    "\n",
    "**Business Need:** Build a multilingual sentiment classifier (English, French, Portuguese) that classifies tweets as:\n",
    "- Positive\n",
    "- Negative\n",
    "\n",
    "This will enable:\n",
    "- Monitoring brand perception\n",
    "- Tracking political sentiment\n",
    "- Analyzing feedback across diverse markets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pwW12qRKrBql"
   },
   "source": [
    "## 2. Data Understanding\n",
    "\n",
    "You have 3 datasets:\n",
    "- ðŸ‡¬ðŸ‡§ English: 100k+ samples with text and sentiment\n",
    "- ðŸ‡«ðŸ‡· French: ~9 lakh samples, but lacks \"neutral\"\n",
    "- ðŸ‡µðŸ‡¹ Portuguese: ~6 lakh samples\n",
    "\n",
    "Challenges:\n",
    "- Label format inconsistencies (e.g., 0/1/2, strings)\n",
    "- Extra columns\n",
    "- Missing/imbalanced classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "YwYhjkrLsNLO"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5dy8m54QshPv"
   },
   "source": [
    "## 3. Data Preparation\n",
    "\n",
    "Steps:\n",
    "- Clean column formats\n",
    "- Drop extra columns\n",
    "- Map labels to 'positive'/'negative'\n",
    "- Remove neutral samples\n",
    "- Stratified downsample to 65k per language\n",
    "- Combine into 195k multilingual dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NFsn3rKusYjG",
    "outputId": "292bbeb9-af7e-4fcb-af07-ae19a9216a9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "base_path = '/content/drive/My Drive/MS-AAI/AAI-510-IN3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "fDYQvvFMsqxY"
   },
   "outputs": [],
   "source": [
    "def stratified_downsample(df, sample_size):\n",
    "    label_dist = df['label'].value_counts(normalize=True).to_dict()\n",
    "    samples = []\n",
    "    for label, ratio in label_dist.items():\n",
    "        n = int(sample_size * ratio)\n",
    "        part = df[df['label'] == label].sample(n=n, random_state=42)\n",
    "        samples.append(part)\n",
    "    return pd.concat(samples).sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "8WMDdlbwssqc"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ------------------------\n",
    "# ðŸŸ¢ 1. Portuguese Dataset\n",
    "# ------------------------\n",
    "\n",
    "def process_portuguese(input_path, output_path):\n",
    "    df = pd.read_csv(input_path, sep=';', quoting=3, encoding='utf-8', on_bad_lines='skip')\n",
    "\n",
    "    # Keep only necessary columns\n",
    "    df = df[['tweet_text', 'sentiment']]\n",
    "    df.columns = ['text', 'label']\n",
    "    df['language'] = 'pt'\n",
    "\n",
    "    label_map = {\n",
    "        '0': 'negative', '1': 'positive', '2': 'neutral',\n",
    "        0: 'negative', 1: 'positive', 2: 'neutral'\n",
    "    }\n",
    "    df['label'] = df['label'].map(label_map)\n",
    "    df = df[df['label'].isin(['positive', 'negative'])]\n",
    "    df.info()\n",
    "    # Stratified downsample\n",
    "    sampled = stratified_downsample(df, 65000)\n",
    "    sampled.to_csv(output_path, index=False)\n",
    "    print(f\"âœ… Portuguese dataset saved: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ITHE_ZL9susJ"
   },
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# ðŸŸ¢ 2. English Dataset\n",
    "# --------------------\n",
    "\n",
    "def process_english(input_path, output_path):\n",
    "    df = pd.read_csv(input_path)\n",
    "    df = df[['Text', 'Label']]\n",
    "    df.columns = ['text', 'label']\n",
    "    df['language'] = 'en'\n",
    "\n",
    "    df['label'] = df['label'].astype(str).str.lower().str.strip()\n",
    "    df = df[df['label'].isin(['positive', 'negative'])]\n",
    "\n",
    "    sampled = stratified_downsample(df, 65000)\n",
    "    sampled.to_csv(output_path, index=False)\n",
    "    print(f\"âœ… English dataset saved: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "OQp_ECsjswRb"
   },
   "outputs": [],
   "source": [
    "# -------------------\n",
    "# ðŸŸ¢ 3. French Dataset\n",
    "# -------------------\n",
    "\n",
    "def process_french(input_path, output_path):\n",
    "    df = pd.read_csv(input_path)\n",
    "    df = df[['text', 'label']]\n",
    "    df.columns = ['text', 'label']\n",
    "    df['language'] = 'fr'\n",
    "\n",
    "    label_map = {\n",
    "        '0': 'negative', '1': 'positive', '2': 'neutral',\n",
    "        0: 'negative', 1: 'positive', 2: 'neutral'\n",
    "    }\n",
    "    df['label'] = df['label'].map(label_map)\n",
    "    df = df[df['label'].isin(['positive', 'negative'])]\n",
    "\n",
    "    sampled = stratified_downsample(df, 65000)\n",
    "    sampled.to_csv(output_path, index=False)\n",
    "    print(f\"âœ… French dataset saved: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x7_fhiPLsx_V",
    "outputId": "d1da11be-d0c4-4eba-d0d2-6a0f586f5704"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 65470 entries, 0 to 65469\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   text      65470 non-null  object\n",
      " 1   label     65470 non-null  object\n",
      " 2   language  65470 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 2.0+ MB\n",
      "âœ… Portuguese dataset saved: /content/drive/My Drive/MS-AAI/AAI-510-IN3/portuguese_cleaned_65k.csv\n",
      "âœ… English dataset saved: /content/drive/My Drive/MS-AAI/AAI-510-IN3/english_cleaned_65k.csv\n",
      "âœ… French dataset saved: /content/drive/My Drive/MS-AAI/AAI-510-IN3/french_cleaned_65k.csv\n"
     ]
    }
   ],
   "source": [
    "process_portuguese(f\"{base_path}/portuguese.csv\", f\"{base_path}/portuguese_cleaned_65k.csv\")\n",
    "process_english(f\"{base_path}/english.csv\", f\"{base_path}/english_cleaned_65k.csv\")\n",
    "process_french(f\"{base_path}/french.csv\", f\"{base_path}/french_cleaned_65k.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OZZ5DLU5s0Eb",
    "outputId": "2174dc83-301a-4b9d-c1b6-89dfc07be32b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Combined dataset saved: multilingual_sentiment_195k.csv\n"
     ]
    }
   ],
   "source": [
    "en = pd.read_csv(f\"{base_path}/english_cleaned_65k.csv\")\n",
    "pt = pd.read_csv(f\"{base_path}/portuguese_cleaned_65k.csv\")\n",
    "fr = pd.read_csv(f\"{base_path}/french_cleaned_65k.csv\")\n",
    "\n",
    "df_all = pd.concat([en, pt, fr])\n",
    "df_all = df_all.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "df_all.to_csv(f\"{base_path}/multilingual_sentiment_195k.csv\", index=False)\n",
    "\n",
    "print(\"âœ… Combined dataset saved: multilingual_sentiment_195k.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Dkwyr8Hfs1uZ"
   },
   "outputs": [],
   "source": [
    "# Encode labels\n",
    "df_all['labels'] = df_all['label'].map({'negative': 0, 'positive': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "-gVbK_ejxgGv",
    "outputId": "5b0acc07-87a7-44ac-e0da-2392c917d417"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "dataframe",
       "variable_name": "df_all"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-51cb0d86-86f7-4da0-9b8a-cef006a63ffe\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>language</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Que daora acho que meu tÃ­tulo foi cancelado :)...</td>\n",
       "      <td>positive</td>\n",
       "      <td>pt</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ok je ferai cela aussi, j'aurai une lecture de...</td>\n",
       "      <td>positive</td>\n",
       "      <td>fr</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mes cheveux sont Ã©loquents! -___- je me sens c...</td>\n",
       "      <td>positive</td>\n",
       "      <td>fr</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@cutesvantae @BTS_twt YAAAA :(((( eu amo demai...</td>\n",
       "      <td>negative</td>\n",
       "      <td>pt</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pourquoi pensez-vous que vous avez vÃ©cu en Aus...</td>\n",
       "      <td>negative</td>\n",
       "      <td>fr</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194992</th>\n",
       "      <td>JÃ¡ agora :) https://t.co/rPMwcGTc0C</td>\n",
       "      <td>positive</td>\n",
       "      <td>pt</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194993</th>\n",
       "      <td>se alguÃ©m souber de desconto pra comprar um pa...</td>\n",
       "      <td>negative</td>\n",
       "      <td>pt</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194994</th>\n",
       "      <td>Joyeux anniversaire pour minuit. Je suis conte...</td>\n",
       "      <td>positive</td>\n",
       "      <td>fr</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194995</th>\n",
       "      <td>- plus de photos de plantation de riz (prises ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>fr</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194996</th>\n",
       "      <td>eu tÃ´ passando muito mal :(</td>\n",
       "      <td>negative</td>\n",
       "      <td>pt</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>194996 rows Ã— 4 columns</p>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-51cb0d86-86f7-4da0-9b8a-cef006a63ffe')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-51cb0d86-86f7-4da0-9b8a-cef006a63ffe button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-51cb0d86-86f7-4da0-9b8a-cef006a63ffe');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-da24097b-3a71-4550-952d-c665a83a0bd0\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-da24097b-3a71-4550-952d-c665a83a0bd0')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-da24097b-3a71-4550-952d-c665a83a0bd0 button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "  <div id=\"id_97fa6723-716d-4e35-990f-206dec982a35\">\n",
       "    <style>\n",
       "      .colab-df-generate {\n",
       "        background-color: #E8F0FE;\n",
       "        border: none;\n",
       "        border-radius: 50%;\n",
       "        cursor: pointer;\n",
       "        display: none;\n",
       "        fill: #1967D2;\n",
       "        height: 32px;\n",
       "        padding: 0 0 0 0;\n",
       "        width: 32px;\n",
       "      }\n",
       "\n",
       "      .colab-df-generate:hover {\n",
       "        background-color: #E2EBFA;\n",
       "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "        fill: #174EA6;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate {\n",
       "        background-color: #3B4455;\n",
       "        fill: #D2E3FC;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate:hover {\n",
       "        background-color: #434B5C;\n",
       "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "        fill: #FFFFFF;\n",
       "      }\n",
       "    </style>\n",
       "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_all')\"\n",
       "            title=\"Generate code using this dataframe.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    <script>\n",
       "      (() => {\n",
       "      const buttonEl =\n",
       "        document.querySelector('#id_97fa6723-716d-4e35-990f-206dec982a35 button.colab-df-generate');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      buttonEl.onclick = () => {\n",
       "        google.colab.notebook.generateWithVariable('df_all');\n",
       "      }\n",
       "      })();\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                                     text     label language  \\\n",
       "0       Que daora acho que meu tÃ­tulo foi cancelado :)...  positive       pt   \n",
       "1       Ok je ferai cela aussi, j'aurai une lecture de...  positive       fr   \n",
       "2       Mes cheveux sont Ã©loquents! -___- je me sens c...  positive       fr   \n",
       "3       @cutesvantae @BTS_twt YAAAA :(((( eu amo demai...  negative       pt   \n",
       "4       Pourquoi pensez-vous que vous avez vÃ©cu en Aus...  negative       fr   \n",
       "...                                                   ...       ...      ...   \n",
       "194992                JÃ¡ agora :) https://t.co/rPMwcGTc0C  positive       pt   \n",
       "194993  se alguÃ©m souber de desconto pra comprar um pa...  negative       pt   \n",
       "194994  Joyeux anniversaire pour minuit. Je suis conte...  positive       fr   \n",
       "194995  - plus de photos de plantation de riz (prises ...  positive       fr   \n",
       "194996                        eu tÃ´ passando muito mal :(  negative       pt   \n",
       "\n",
       "        labels  \n",
       "0            1  \n",
       "1            1  \n",
       "2            1  \n",
       "3            0  \n",
       "4            0  \n",
       "...        ...  \n",
       "194992       1  \n",
       "194993       0  \n",
       "194994       1  \n",
       "194995       1  \n",
       "194996       0  \n",
       "\n",
       "[194996 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all = df_all.dropna()\n",
    "df_all\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j3LTn083s5hY"
   },
   "source": [
    "## 4. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 540
    },
    "id": "o4xsSepRs3O_",
    "outputId": "8d28eb5f-cc66-43af-af8f-446b319883c9"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_readable\n",
      "Negative (0)    97579\n",
      "Positive (1)    97417\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Mapping numeric labels to readable strings\n",
    "label_mapping = {0: \"Negative (0)\", 1: \"Positive (1)\"}\n",
    "df_all['label_readable'] = df_all['labels'].map(label_mapping)\n",
    "\n",
    "# Plot with updated labels\n",
    "sns.countplot(data=df_all, x='label_readable', hue='language')\n",
    "plt.title(\"Sentiment Distribution by Language\")\n",
    "plt.xlabel(\"Sentiment\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "# Print value counts with readable labels\n",
    "print(df_all['label_readable'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xzq_7jKOs8Nv"
   },
   "source": [
    "## 5. Modeling\n",
    "\n",
    "Models to train:\n",
    "1. XLM-RoBERTa Base\n",
    "2. MDeBERTa v3 Base\n",
    "3. DistilBERT Multilingual\n",
    "4. LSTM\n",
    "\n",
    "Each model is trained and evaluated on the same train/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "ba04d60bb0b24c78a26e60cc58d3fcbc",
      "1394871512ef45199f8a07f9a21ed657",
      "4895b3f7a4cd44f18904ebc2f0ee0522",
      "a247602b0abf49dab2b8657e9d5fc17f",
      "47a186282ef04ceeacd9c6c2cda7f6f7",
      "21a00dcaea674f03a8baac3a1e0a7a4d",
      "ba0d1f46194e46ae84b6927c483e3099",
      "2871793c2866486693092a76ffab6231",
      "139cb93f6a204c01bc76966f0279c23d",
      "bd8bbb738a664dfb9265f97ad2c8393c",
      "89a30157f1f54871914d315220de8d0c",
      "8d863a2f92ba4e1e92d3355a4a3776ae",
      "53cf0014c45048d492884822652b9dd1",
      "c507f825a1404e39ac8f4049fb0216a1",
      "00505060fda94efb80c703e1b5c46b2c",
      "3699e34c44c744369f67fe12fe7b35e9",
      "d76410bc4a5c47668dae0dac0d2bd446",
      "b1e68ebd522a4b2186989191b5c7a073",
      "7e264533db6a497bbe509bd8961e8633",
      "9038c069d2cd44ff9954e1fa2c09d29f"
     ]
    },
    "id": "IL_9VHios8uR",
    "outputId": "7d40c020-e1a9-4562-91a0-2ebd3af5f5dc"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba04d60bb0b24c78a26e60cc58d3fcbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pRE60Y_Zwm5v"
   },
   "source": [
    "### Split the data into traning and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "kkOjLOI5wsaZ"
   },
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df_all, test_size=0.2, stratify=df_all['labels'], random_state=42)\n",
    "\n",
    "train_ds = Dataset.from_pandas(train_df[['text', 'labels']])\n",
    "test_ds = Dataset.from_pandas(test_df[['text', 'labels']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zMu6YdBCwWg1"
   },
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VMQS3bH64Jdg",
    "outputId": "82bb1fef-a515-4911-8b25-f132f2698996"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m2194/2194\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 10ms/step - accuracy: 0.7792 - loss: 0.4303 - val_accuracy: 0.8419 - val_loss: 0.3301\n",
      "Epoch 2/10\n",
      "\u001b[1m2194/2194\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 9ms/step - accuracy: 0.8717 - loss: 0.2813 - val_accuracy: 0.8416 - val_loss: 0.3356\n",
      "Epoch 3/10\n",
      "\u001b[1m2194/2194\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 9ms/step - accuracy: 0.8916 - loss: 0.2368 - val_accuracy: 0.8413 - val_loss: 0.3552\n",
      "\u001b[1m1219/1219\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=2,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "max_len = 100\n",
    "tokenizer = Tokenizer(num_words=20000)\n",
    "tokenizer.fit_on_texts(train_df['text'])\n",
    "\n",
    "X_train = pad_sequences(tokenizer.texts_to_sequences(train_df['text']), maxlen=max_len)\n",
    "X_test = pad_sequences(tokenizer.texts_to_sequences(test_df['text']), maxlen=max_len)\n",
    "y_train = to_categorical(train_df['labels'])\n",
    "y_test = to_categorical(test_df['labels'])\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(20000, 128))\n",
    "model.add(LSTM(64, dropout=0.2))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=64,callbacks=[early_stop], validation_split=0.1)\n",
    "\n",
    "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "results = {}\n",
    "results[\"LSTM\"] = {\n",
    "    \"report\": classification_report(y_true, y_pred, target_names=[\"negative\", \"positive\"], output_dict=True)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "N9OhHKkH9Nhd"
   },
   "outputs": [],
   "source": [
    "### Save the model and tokenizer\n",
    "from tensorflow.keras.preprocessing.text import tokenizer_from_json\n",
    "import json\n",
    "\n",
    "# After training\n",
    "model.save(\"lstm_sentiment_model.keras\")\n",
    "\n",
    "\n",
    "# Convert tokenizer to JSON\n",
    "tokenizer_json = tokenizer.to_json()\n",
    "\n",
    "# Save to file\n",
    "with open(\"lstm_tokenizer.json\", \"w\") as f:\n",
    "    f.write(tokenizer_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5O5f4yEN4OXi",
    "outputId": "8ef386d4-3d21-4ec0-ba5d-40ac1c571fa5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1219/1219\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step\n",
      "ðŸ“Š Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.86      0.85     19516\n",
      "    positive       0.85      0.83      0.84     19484\n",
      "\n",
      "    accuracy                           0.84     39000\n",
      "   macro avg       0.84      0.84      0.84     39000\n",
      "weighted avg       0.84      0.84      0.84     39000\n",
      "\n",
      "âœ… Accuracy: 0.8434\n",
      "âœ… F1 Score: 0.8409\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "\n",
    "# 1. Predict probabilities\n",
    "y_pred_probs = model.predict(X_test)\n",
    "\n",
    "# 2. Convert probabilities to predicted class labels\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# 3. Convert one-hot true labels to class indices\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "# 4. Print classification report\n",
    "print(\"ðŸ“Š Classification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=[\"negative\", \"positive\"]))\n",
    "\n",
    "# 5. Optionally compute and print accuracy or F1\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "print(f\"âœ… Accuracy: {acc:.4f}\")\n",
    "print(f\"âœ… F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hNwNxqeG_L3n"
   },
   "source": [
    "### Lightweight BERT Multilingual Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 437,
     "referenced_widgets": [
      "8630d293a6d84d6689f4a234e951c11a",
      "29d6165f7a3b4cb797cb6c04952921ff",
      "11e3d370712340f28c992517f4737a69",
      "286b09b5ac094e85b9ad1a2b025dc722",
      "cf98aaf9a99a49b7a6abf7c640022eb4",
      "7220a329412a401e9cffbdeec4fb0754",
      "c8b44e31d2044407b1e97e121f172777",
      "e5c785eb5a384a4e95a4e6065389aed2",
      "2262fb74ec6744f4b569b690de7e150b",
      "a980af3b795f4dd2b4cc2b89afdfe779",
      "fb8e13e643624aa693aaf26641e298e5",
      "f5be1160b237442ab5b3c0d8fb2af2a6",
      "680831e06d7c4d899399cffd29a0b32c",
      "1eb764a3779a45988e6626094878d205",
      "7909932f92f54b3fb070f6e5460f0a17",
      "ed65b654df114a1dbfa0f612f83439a2",
      "24d310b840ba4e7b93b331186534344d",
      "1a275384dec64e82ace7d4186141ffb4",
      "de6636911d714565935a048dbff495b3",
      "66a588683fe1488eafc940168ae2e7e0",
      "04b9cd68c6614aa1a2ff4b65f14f1496",
      "00b2d6dc50934559a619a4c58d221d48",
      "43e1dfff558e427abfb1a3b67550b924",
      "bda694bc1e984ba9aadd7c8ba8fcc81c",
      "9ea24ea2f3a84240a93ef9483296214d",
      "2a3cd27e92874eb4a7a997811b89f2c0",
      "85de9016cb724de58fae0d2906d3254a",
      "15ba5c79aa414de89c970b9cb74687e7",
      "322e1f3f23f145859628dc49be36d64c",
      "802de672f01a449087071b818bc876c6",
      "e9aa9e0fdab542418400c64b473c7772",
      "6fe4ee99607f495f85024fe5b724b7bf",
      "ffddfc2d0c8d4259afb2c70e04c1f68c",
      "bf0b3249efa447c38d5d6f14ca92c71e",
      "b2d0d02c468f4ac1bca9889201e5e6c5",
      "c1137d681b1c41d9b1953c9620832901",
      "b6a3b054d7a04355a10b1dc0e2c1b817",
      "05214e3294e14b3fa1a299f2d47d9ffb",
      "e76a10cc224140fe86323e857e2510b7",
      "fce3cbee38e9415b9bcee50ff7117cf9",
      "02c5f4ace1b4429780185344197f1339",
      "c119cbb73a184e27a4d5c6ed8038fb9e",
      "89f65c92c373442ca7aa09b8904bc8e2",
      "7bdee0163c174432bcee5d605b320247"
     ]
    },
    "id": "7UnaRxbT_K_n",
    "outputId": "5ae13123-72a9-4097-a3cf-5b45f310fa99"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8630d293a6d84d6689f4a234e951c11a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/383 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5be1160b237442ab5b3c0d8fb2af2a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43e1dfff558e427abfb1a3b67550b924",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/155996 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf0b3249efa447c38d5d6f14ca92c71e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/39000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset: Dataset({\n",
      "    features: ['text', 'labels', '__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 155996\n",
      "})\n",
      "Test dataset: Dataset({\n",
      "    features: ['text', 'labels', '__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 39000\n",
      "})\n",
      "Sample train data: {'text': '@canoglan he sil sil :)', 'labels': 1, '__index_level_0__': 139250, 'input_ids': [101, 1030, 2064, 8649, 5802, 2002, 9033, 2140, 9033, 2140, 1024, 1007, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import pandas as pd\n",
    "\n",
    "# Cell 2: Your existing tokenization code\n",
    "from transformers import AutoTokenizer\n",
    "#model_name = \"distilbert-base-multilingual-cased\"\n",
    "model_name =\"google/bert_uncased_L-4_H-256_A-4\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], truncation=True, padding=True)\n",
    "\n",
    "train_ds = train_ds.map(tokenize, batched=True)\n",
    "test_ds = test_ds.map(tokenize, batched=True)\n",
    "\n",
    "# Cell 3: Check dataset info\n",
    "print(\"Train dataset:\", train_ds)\n",
    "print(\"Test dataset:\", test_ds)\n",
    "print(\"Sample train data:\", train_ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 271,
     "referenced_widgets": [
      "d42f88b292364f73ab4a83f1866a2a1e",
      "b04756757b9b4a01965b77679c66cbbb",
      "76892765d09b4e7790154af394859eeb",
      "f1a29de3bc3947eaadb682eaf4c3fe33",
      "48af9232fb7b4492a24324dfce57dc81",
      "9a93c7db041544cbade68c84653ccca6",
      "e44c8235ef4f4534b47d953adb50963e",
      "d7139e7804984f90a4c110220c126513",
      "82b9579053d54e8f86a0c1efc72c4511",
      "e4d3236c6b9d4477967782fded034d45",
      "aed354f510f646df96b725f5824a2d5d",
      "bc939d98e8234ff5bbd1112fe1224029",
      "5408dd055ee14be2b2753610f60b5266",
      "4929adc21a1b42eeb422e08a60279a9e",
      "67c9f4aac22845db9cd548c4fed25e09",
      "2b0f2a6da1ae45889f5d78a4c4eac57d",
      "91db7b4374ca485382562baf54325060",
      "c360532d516e430b989205a9ba1e6bfb",
      "929d06cf125f41498aa00ea3406749d7",
      "23b2a9d02ef94c31b6a660d02b69215d",
      "12badee4f62140898a3b462d693aa457",
      "d39bd35485864058ab09943ebdcd7933"
     ]
    },
    "id": "lXODB2DVCHJO",
    "outputId": "58cb70fe-6266-49d6-b821-14f4002a7115"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labels: 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d42f88b292364f73ab4a83f1866a2a1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/45.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-4_H-256_A-4 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipython-input-19-1856717858.py:54: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original datasets preserved, working with cloned datasets for training\n",
      "Dataset format updated\n",
      "Training arguments configured\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc939d98e8234ff5bbd1112fe1224029",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/45.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer initialized\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "num_labels = len(df_all['labels'].unique())\n",
    "print(f\"Number of labels: {num_labels}\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_labels\n",
    ")\n",
    "\n",
    "# Set format for training\n",
    "train_ds.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "test_ds.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "print(\"Original datasets preserved, working with cloned datasets for training\")\n",
    "\n",
    "print(\"Dataset format updated\")\n",
    "\n",
    "# Cell 6: Data collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Cell 7: Define metrics function\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "# Cell 8: Training arguments (compatible with older transformers)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results_bert',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=50,\n",
    "    seed=42,\n",
    "    save_steps=1000,\n",
    "    eval_steps=1000,\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "print(\"Training arguments configured\")\n",
    "\n",
    "# Cell 9: Initialize trainer (using cloned datasets)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized\")\n",
    "\n",
    "# Cell 10: Check device and start training\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "vPqOSp-H_cvv",
    "outputId": "25513734-f394-4206-e7cb-897518fc24ad"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
       "            function loadScript(url) {\n",
       "            return new Promise(function(resolve, reject) {\n",
       "                let newScript = document.createElement(\"script\");\n",
       "                newScript.onerror = reject;\n",
       "                newScript.onload = resolve;\n",
       "                document.body.appendChild(newScript);\n",
       "                newScript.src = url;\n",
       "            });\n",
       "            }\n",
       "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
       "            const iframe = document.createElement('iframe')\n",
       "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
       "            document.body.appendChild(iframe)\n",
       "            const handshake = new Postmate({\n",
       "                container: iframe,\n",
       "                url: 'https://wandb.ai/authorize'\n",
       "            });\n",
       "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
       "            handshake.then(function(child) {\n",
       "                child.on('authorize', data => {\n",
       "                    clearTimeout(timeout)\n",
       "                    resolve(data)\n",
       "                });\n",
       "            });\n",
       "            })\n",
       "        });\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
      "wandb: Paste an API key from your profile and hit enter:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmnair\u001b[0m (\u001b[33mmnair-university-of-san-diego\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20250621_041733-df3wb927</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mnair-university-of-san-diego/huggingface/runs/df3wb927' target=\"_blank\">./results_bert</a></strong> to <a href='https://wandb.ai/mnair-university-of-san-diego/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mnair-university-of-san-diego/huggingface' target=\"_blank\">https://wandb.ai/mnair-university-of-san-diego/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mnair-university-of-san-diego/huggingface/runs/df3wb927' target=\"_blank\">https://wandb.ai/mnair-university-of-san-diego/huggingface/runs/df3wb927</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14625' max='14625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14625/14625 09:36, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.673500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.641700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.591600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.545400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.481900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.407700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.347500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.334000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.326600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.289700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.277000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.262000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.270600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.253500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.262000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.255300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.234800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.238000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.227500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.257300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.244000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.233800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.241800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.219000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.211400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.220900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.208900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.221700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.221000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.216200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.211700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.180800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.211600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.215200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.205900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.226000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.223900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.194900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.207900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.191100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.216000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.217600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.210500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.198600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.215500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.195500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.206200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.192100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.210000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.201500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.205600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.215700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.199600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.203600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.207700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.193700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>0.224300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.200700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>0.212200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.203100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>0.195700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.189500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>0.184700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.187800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>0.200800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.177200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>0.182900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.213600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>0.211000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.197200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>0.195700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.197400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>0.195100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.198800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>0.189300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.195500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3850</td>\n",
       "      <td>0.179000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.192700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3950</td>\n",
       "      <td>0.178200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.186800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4050</td>\n",
       "      <td>0.192300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.194200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4150</td>\n",
       "      <td>0.173700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.194200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4250</td>\n",
       "      <td>0.194000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.186800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4350</td>\n",
       "      <td>0.195500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.201700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4450</td>\n",
       "      <td>0.209000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.212200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4550</td>\n",
       "      <td>0.176600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.197800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4650</td>\n",
       "      <td>0.193100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.177400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4750</td>\n",
       "      <td>0.187200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.191900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4850</td>\n",
       "      <td>0.189400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.181000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4950</td>\n",
       "      <td>0.186800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.173100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5050</td>\n",
       "      <td>0.178000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>0.183500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5150</td>\n",
       "      <td>0.181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.193300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5250</td>\n",
       "      <td>0.163400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>0.188100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5350</td>\n",
       "      <td>0.160300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.164300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5450</td>\n",
       "      <td>0.165700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.173000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5550</td>\n",
       "      <td>0.170100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.168900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5650</td>\n",
       "      <td>0.166200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>0.182200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5750</td>\n",
       "      <td>0.182100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.168300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5850</td>\n",
       "      <td>0.196500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>0.155000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5950</td>\n",
       "      <td>0.180200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.167200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6050</td>\n",
       "      <td>0.189100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>0.159300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6150</td>\n",
       "      <td>0.178400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.161400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6250</td>\n",
       "      <td>0.174800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>0.179700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6350</td>\n",
       "      <td>0.194000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.161700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6450</td>\n",
       "      <td>0.190200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.167500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6550</td>\n",
       "      <td>0.170800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.172400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6650</td>\n",
       "      <td>0.180700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>0.161700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6750</td>\n",
       "      <td>0.156100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.157000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6850</td>\n",
       "      <td>0.171300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>0.172500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6950</td>\n",
       "      <td>0.180100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.184100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7050</td>\n",
       "      <td>0.165500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>0.167700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7150</td>\n",
       "      <td>0.172800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>0.180900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7250</td>\n",
       "      <td>0.172200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>0.182200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7350</td>\n",
       "      <td>0.182700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>0.167700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7450</td>\n",
       "      <td>0.160100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.171200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7550</td>\n",
       "      <td>0.178800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>0.173500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7650</td>\n",
       "      <td>0.171500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7700</td>\n",
       "      <td>0.188900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7750</td>\n",
       "      <td>0.166400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>0.164600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7850</td>\n",
       "      <td>0.176300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7900</td>\n",
       "      <td>0.154700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7950</td>\n",
       "      <td>0.151100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.163700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8050</td>\n",
       "      <td>0.159700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8100</td>\n",
       "      <td>0.173300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8150</td>\n",
       "      <td>0.165700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>0.174100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8250</td>\n",
       "      <td>0.188700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8300</td>\n",
       "      <td>0.167700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8350</td>\n",
       "      <td>0.178500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>0.159200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8450</td>\n",
       "      <td>0.151500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.198700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8550</td>\n",
       "      <td>0.180300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>0.168000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8650</td>\n",
       "      <td>0.184800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8700</td>\n",
       "      <td>0.151900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8750</td>\n",
       "      <td>0.192300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>0.179200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8850</td>\n",
       "      <td>0.181300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8900</td>\n",
       "      <td>0.168500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8950</td>\n",
       "      <td>0.168600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.170300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9050</td>\n",
       "      <td>0.187600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9100</td>\n",
       "      <td>0.166000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9150</td>\n",
       "      <td>0.177800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>0.162000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9250</td>\n",
       "      <td>0.169000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9300</td>\n",
       "      <td>0.161200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9350</td>\n",
       "      <td>0.158600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9400</td>\n",
       "      <td>0.174900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9450</td>\n",
       "      <td>0.168300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.185400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9550</td>\n",
       "      <td>0.165400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>0.184100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9650</td>\n",
       "      <td>0.173700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9700</td>\n",
       "      <td>0.181100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9750</td>\n",
       "      <td>0.177000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9800</td>\n",
       "      <td>0.156700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9850</td>\n",
       "      <td>0.150200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9900</td>\n",
       "      <td>0.163200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9950</td>\n",
       "      <td>0.163300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.153300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10050</td>\n",
       "      <td>0.142200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10100</td>\n",
       "      <td>0.188400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10150</td>\n",
       "      <td>0.173500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10200</td>\n",
       "      <td>0.161300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10250</td>\n",
       "      <td>0.169700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10300</td>\n",
       "      <td>0.158600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10350</td>\n",
       "      <td>0.152800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10400</td>\n",
       "      <td>0.141100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10450</td>\n",
       "      <td>0.175700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.147700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10550</td>\n",
       "      <td>0.149300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10600</td>\n",
       "      <td>0.164600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10650</td>\n",
       "      <td>0.151600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10700</td>\n",
       "      <td>0.161100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10750</td>\n",
       "      <td>0.165800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10800</td>\n",
       "      <td>0.147500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10850</td>\n",
       "      <td>0.154500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10900</td>\n",
       "      <td>0.152100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10950</td>\n",
       "      <td>0.156800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.170100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11050</td>\n",
       "      <td>0.154600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11100</td>\n",
       "      <td>0.172300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11150</td>\n",
       "      <td>0.168600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11200</td>\n",
       "      <td>0.123000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11250</td>\n",
       "      <td>0.145200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11300</td>\n",
       "      <td>0.171300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11350</td>\n",
       "      <td>0.168500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11400</td>\n",
       "      <td>0.154800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11450</td>\n",
       "      <td>0.151600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.157400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11550</td>\n",
       "      <td>0.149200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11600</td>\n",
       "      <td>0.172100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11650</td>\n",
       "      <td>0.142400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11700</td>\n",
       "      <td>0.140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11750</td>\n",
       "      <td>0.130900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11800</td>\n",
       "      <td>0.162500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11850</td>\n",
       "      <td>0.140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11900</td>\n",
       "      <td>0.153800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11950</td>\n",
       "      <td>0.173000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.150800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12050</td>\n",
       "      <td>0.154600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12100</td>\n",
       "      <td>0.154000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12150</td>\n",
       "      <td>0.152000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12200</td>\n",
       "      <td>0.166500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12250</td>\n",
       "      <td>0.151100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12300</td>\n",
       "      <td>0.153800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12350</td>\n",
       "      <td>0.168200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12400</td>\n",
       "      <td>0.151300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12450</td>\n",
       "      <td>0.178200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.144400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12550</td>\n",
       "      <td>0.149400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12600</td>\n",
       "      <td>0.145600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12650</td>\n",
       "      <td>0.162500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12700</td>\n",
       "      <td>0.156300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12750</td>\n",
       "      <td>0.152700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12800</td>\n",
       "      <td>0.154400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12850</td>\n",
       "      <td>0.152600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12900</td>\n",
       "      <td>0.147000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12950</td>\n",
       "      <td>0.151600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.148600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13050</td>\n",
       "      <td>0.154300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13100</td>\n",
       "      <td>0.144100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13150</td>\n",
       "      <td>0.160300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13200</td>\n",
       "      <td>0.142500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13250</td>\n",
       "      <td>0.142700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13300</td>\n",
       "      <td>0.166800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13350</td>\n",
       "      <td>0.173600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13400</td>\n",
       "      <td>0.150300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13450</td>\n",
       "      <td>0.169600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.152100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13550</td>\n",
       "      <td>0.162400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13600</td>\n",
       "      <td>0.167800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13650</td>\n",
       "      <td>0.147600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13700</td>\n",
       "      <td>0.161300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13750</td>\n",
       "      <td>0.152900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13800</td>\n",
       "      <td>0.151000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13850</td>\n",
       "      <td>0.175200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13900</td>\n",
       "      <td>0.160800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13950</td>\n",
       "      <td>0.152200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.141400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14050</td>\n",
       "      <td>0.144300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14100</td>\n",
       "      <td>0.171600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14150</td>\n",
       "      <td>0.168200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14200</td>\n",
       "      <td>0.147800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14250</td>\n",
       "      <td>0.160400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14300</td>\n",
       "      <td>0.137200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14350</td>\n",
       "      <td>0.168900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14400</td>\n",
       "      <td>0.143700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14450</td>\n",
       "      <td>0.160500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.152600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14550</td>\n",
       "      <td>0.132400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14600</td>\n",
       "      <td>0.156600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=14625, training_loss=0.18762612442277435, metrics={'train_runtime': 598.2384, 'train_samples_per_second': 782.277, 'train_steps_per_second': 24.447, 'total_flos': 2309561241682752.0, 'train_loss': 0.18762612442277435, 'epoch': 3.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Starting training...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "id": "Srf9EcpM_ej3",
    "outputId": "ead23b20-6f03-4cf8-d8d8-ca2c7b79057b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='610' max='610' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [610/610 00:11]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results:\n",
      "  eval_loss: 0.1800\n",
      "  eval_accuracy: 0.9186\n",
      "  eval_f1: 0.9186\n",
      "  eval_precision: 0.9187\n",
      "  eval_recall: 0.9186\n",
      "  eval_runtime: 11.4807\n",
      "  eval_samples_per_second: 3397.0060\n",
      "  eval_steps_per_second: 53.1330\n",
      "  epoch: 3.0000\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating model...\")\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Evaluation results:\")\n",
    "for key, value in eval_results.items():\n",
    "    print(f\"  {key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nOP2Eu7a92iH"
   },
   "source": [
    "## Testing the model (Inference)\n",
    "\n",
    "1. LSTM\n",
    "2. Lightweight BERT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QiKHxGRw-ujm",
    "outputId": "6eeadf99-0962-4a9a-cc54-e4893cfb75e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step\n",
      "Text: I love this product, it works perfectly!\n",
      "âœ… Predicted: Positive | ðŸŽ¯ Expected: positive\n",
      "--------------------------------------------------------------------------------\n",
      "Text: Worst experience ever. Totally disappointed.\n",
      "âœ… Predicted: Negative | ðŸŽ¯ Expected: negative\n",
      "--------------------------------------------------------------------------------\n",
      "Text: Absolutely fantastic! Highly recommend it.\n",
      "âœ… Predicted: Positive | ðŸŽ¯ Expected: positive\n",
      "--------------------------------------------------------------------------------\n",
      "Text: Not worth the price at all.\n",
      "âœ… Predicted: Negative | ðŸŽ¯ Expected: negative\n",
      "--------------------------------------------------------------------------------\n",
      "Text: Great service and quick delivery!\n",
      "âœ… Predicted: Negative | ðŸŽ¯ Expected: positive\n",
      "--------------------------------------------------------------------------------\n",
      "Text: The product stopped working after a week.\n",
      "âœ… Predicted: Negative | ðŸŽ¯ Expected: negative\n",
      "--------------------------------------------------------------------------------\n",
      "Text: Superb quality and amazing value.\n",
      "âœ… Predicted: Positive | ðŸŽ¯ Expected: positive\n",
      "--------------------------------------------------------------------------------\n",
      "Text: Terrible customer support, I wonâ€™t buy again.\n",
      "âœ… Predicted: Negative | ðŸŽ¯ Expected: negative\n",
      "--------------------------------------------------------------------------------\n",
      "Text: Exceeded my expectations in every way.\n",
      "âœ… Predicted: Negative | ðŸŽ¯ Expected: positive\n",
      "--------------------------------------------------------------------------------\n",
      "Text: I regret buying this. Waste of money.\n",
      "âœ… Predicted: Positive | ðŸŽ¯ Expected: negative\n",
      "--------------------------------------------------------------------------------\n",
      "Text: Ce produit est incroyable, je l'adore !\n",
      "âœ… Predicted: Positive | ðŸŽ¯ Expected: positive\n",
      "--------------------------------------------------------------------------------\n",
      "Text: TrÃ¨s mauvaise qualitÃ©, je suis dÃ©Ã§u.\n",
      "âœ… Predicted: Negative | ðŸŽ¯ Expected: negative\n",
      "--------------------------------------------------------------------------------\n",
      "Text: Service rapide et efficace, merci !\n",
      "âœ… Predicted: Positive | ðŸŽ¯ Expected: positive\n",
      "--------------------------------------------------------------------------------\n",
      "Text: Je ne recommande pas cet article.\n",
      "âœ… Predicted: Negative | ðŸŽ¯ Expected: negative\n",
      "--------------------------------------------------------------------------------\n",
      "Text: Le personnel Ã©tait trÃ¨s gentil et serviable.\n",
      "âœ… Predicted: Positive | ðŸŽ¯ Expected: positive\n",
      "--------------------------------------------------------------------------------\n",
      "Text: C'est une perte de temps et d'argent.\n",
      "âœ… Predicted: Negative | ðŸŽ¯ Expected: negative\n",
      "--------------------------------------------------------------------------------\n",
      "Text: Livraison rapide et produit conforme.\n",
      "âœ… Predicted: Positive | ðŸŽ¯ Expected: positive\n",
      "--------------------------------------------------------------------------------\n",
      "Text: Le produit est arrivÃ© cassÃ©. TrÃ¨s dÃ©Ã§u.\n",
      "âœ… Predicted: Negative | ðŸŽ¯ Expected: negative\n",
      "--------------------------------------------------------------------------------\n",
      "Text: ExpÃ©rience formidable du dÃ©but Ã  la fin.\n",
      "âœ… Predicted: Positive | ðŸŽ¯ Expected: positive\n",
      "--------------------------------------------------------------------------------\n",
      "Text: Mauvais service client et produit inutilisable.\n",
      "âœ… Predicted: Negative | ðŸŽ¯ Expected: negative\n",
      "--------------------------------------------------------------------------------\n",
      "Text: Adorei este produto, funciona perfeitamente!\n",
      "âœ… Predicted: Positive | ðŸŽ¯ Expected: positive\n",
      "--------------------------------------------------------------------------------\n",
      "Text: O produto veio com defeito. Decepcionado.\n",
      "âœ… Predicted: Positive | ðŸŽ¯ Expected: negative\n",
      "--------------------------------------------------------------------------------\n",
      "Text: Entrega super rÃ¡pida e produto Ã³timo.\n",
      "âœ… Predicted: Positive | ðŸŽ¯ Expected: positive\n",
      "--------------------------------------------------------------------------------\n",
      "Text: NÃ£o recomendo, pÃ©ssima qualidade.\n",
      "âœ… Predicted: Negative | ðŸŽ¯ Expected: negative\n",
      "--------------------------------------------------------------------------------\n",
      "Text: Excelente atendimento e produto maravilhoso.\n",
      "âœ… Predicted: Positive | ðŸŽ¯ Expected: positive\n",
      "--------------------------------------------------------------------------------\n",
      "Text: DesperdÃ­cio de dinheiro. Arrependido.\n",
      "âœ… Predicted: Negative | ðŸŽ¯ Expected: negative\n",
      "--------------------------------------------------------------------------------\n",
      "Text: Muito satisfeito com a minha compra!\n",
      "âœ… Predicted: Negative | ðŸŽ¯ Expected: positive\n",
      "--------------------------------------------------------------------------------\n",
      "Text: O suporte ao cliente Ã© horrÃ­vel.\n",
      "âœ… Predicted: Negative | ðŸŽ¯ Expected: negative\n",
      "--------------------------------------------------------------------------------\n",
      "Text: Compra perfeita, sem problemas.\n",
      "âœ… Predicted: Negative | ðŸŽ¯ Expected: positive\n",
      "--------------------------------------------------------------------------------\n",
      "Text: Nunca mais compro nesta loja.\n",
      "âœ… Predicted: Positive | ðŸŽ¯ Expected: negative\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 1. LSTM Testing\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import tokenizer_from_json\n",
    "\n",
    "json_file_path = f\"{base_path}/test_samples_multilang.json\"\n",
    "\n",
    "# Step 1: Load test samples from JSON\n",
    "with open(json_file_path, 'r', encoding='utf-8') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "texts = [item['text'] for item in test_data]\n",
    "true_labels = [item['expected_label'] for item in test_data]\n",
    "\n",
    "# Load tokenizer from JSON file\n",
    "with open('lstm_tokenizer.json', 'r') as f:\n",
    "    tokenizer = tokenizer_from_json(f.read())\n",
    "\n",
    "# Step 3: Tokenize and pad the test texts\n",
    "max_len = 100  # Must match what was used during training\n",
    "X_test = tokenizer.texts_to_sequences(texts)\n",
    "X_test = pad_sequences(X_test, maxlen=max_len)\n",
    "\n",
    "# Step 4: Load the trained LSTM model\n",
    "model = load_model(\"lstm_sentiment_model.keras\")\n",
    "\n",
    "# Step 5: Perform predictions\n",
    "y_pred_probs = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# Step 6: Map labels to human-readable form\n",
    "label_map = {0: \"Negative\", 1: \"Positive\"}\n",
    "\n",
    "# Step 7: Print results\n",
    "for i, text in enumerate(texts):\n",
    "    predicted_label = label_map[y_pred[i]]\n",
    "    expected_label = true_labels[i]\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"âœ… Predicted: {predicted_label} | ðŸŽ¯ Expected: {expected_label}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "upt1L4_n4oSY",
    "outputId": "77e49fdb-aa8d-4cdb-b6a1-d338591c3872"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Running inference on test samples...\n",
      "\n",
      "ðŸ“ Text: I love this product, it works perfectly!\n",
      "âœ… True Label: positive\n",
      "ðŸ”® Predicted: Positive (97.8%)\n",
      "------------------------------------------------------------\n",
      "ðŸ“ Text: Worst experience ever. Totally disappointed.\n",
      "âœ… True Label: negative\n",
      "ðŸ”® Predicted: Positive (84.16%)\n",
      "------------------------------------------------------------\n",
      "ðŸ“ Text: Absolutely fantastic! Highly recommend it.\n",
      "âœ… True Label: positive\n",
      "ðŸ”® Predicted: Positive (89.8%)\n",
      "------------------------------------------------------------\n",
      "ðŸ“ Text: Not worth the price at all.\n",
      "âœ… True Label: negative\n",
      "ðŸ”® Predicted: Negative (61.58%)\n",
      "------------------------------------------------------------\n",
      "ðŸ“ Text: Great service and quick delivery!\n",
      "âœ… True Label: positive\n",
      "ðŸ”® Predicted: Positive (94.65%)\n",
      "------------------------------------------------------------\n",
      "ðŸ“ Text: The product stopped working after a week.\n",
      "âœ… True Label: negative\n",
      "ðŸ”® Predicted: Positive (70.96%)\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ Text: Superb quality and amazing value.\n",
      "âœ… True Label: positive\n",
      "ðŸ”® Predicted: Positive (97.7%)\n",
      "------------------------------------------------------------\n",
      "ðŸ“ Text: Terrible customer support, I wonâ€™t buy again.\n",
      "âœ… True Label: negative\n",
      "ðŸ”® Predicted: Negative (99.43%)\n",
      "------------------------------------------------------------\n",
      "ðŸ“ Text: Exceeded my expectations in every way.\n",
      "âœ… True Label: positive\n",
      "ðŸ”® Predicted: Negative (80.42%)\n",
      "------------------------------------------------------------\n",
      "ðŸ“ Text: I regret buying this. Waste of money.\n",
      "âœ… True Label: negative\n",
      "ðŸ”® Predicted: Negative (99.91%)\n",
      "------------------------------------------------------------\n",
      "ðŸ“ Text: Ce produit est incroyable, je l'adore !\n",
      "âœ… True Label: positive\n",
      "ðŸ”® Predicted: Positive (98.67%)\n",
      "------------------------------------------------------------\n",
      "ðŸ“ Text: TrÃ¨s mauvaise qualitÃ©, je suis dÃ©Ã§u.\n",
      "âœ… True Label: negative\n",
      "ðŸ”® Predicted: Negative (98.88%)\n",
      "------------------------------------------------------------\n",
      "ðŸ“ Text: Service rapide et efficace, merci !\n",
      "âœ… True Label: positive\n",
      "ðŸ”® Predicted: Positive (79.68%)\n",
      "------------------------------------------------------------\n",
      "ðŸ“ Text: Je ne recommande pas cet article.\n",
      "âœ… True Label: negative\n",
      "ðŸ”® Predicted: Negative (94.12%)\n",
      "------------------------------------------------------------\n",
      "ðŸ“ Text: Le personnel Ã©tait trÃ¨s gentil et serviable.\n",
      "âœ… True Label: positive\n",
      "ðŸ”® Predicted: Positive (91.81%)\n",
      "------------------------------------------------------------\n",
      "ðŸ“ Text: C'est une perte de temps et d'argent.\n",
      "âœ… True Label: negative\n",
      "ðŸ”® Predicted: Negative (90.36%)\n",
      "------------------------------------------------------------\n",
      "ðŸ“ Text: Livraison rapide et produit conforme.\n",
      "âœ… True Label: positive\n",
      "ðŸ”® Predicted: Positive (91.7%)\n",
      "------------------------------------------------------------\n",
      "ðŸ“ Text: Le produit est arrivÃ© cassÃ©. TrÃ¨s dÃ©Ã§u.\n",
      "âœ… True Label: negative\n",
      "ðŸ”® Predicted: Negative (97.86%)\n",
      "------------------------------------------------------------\n",
      "ðŸ“ Text: ExpÃ©rience formidable du dÃ©but Ã  la fin.\n",
      "âœ… True Label: positive\n",
      "ðŸ”® Predicted: Positive (97.01%)\n",
      "------------------------------------------------------------\n",
      "ðŸ“ Text: Mauvais service client et produit inutilisable.\n",
      "âœ… True Label: negative\n",
      "ðŸ”® Predicted: Negative (79.12%)\n",
      "------------------------------------------------------------\n",
      "ðŸ“ Text: Adorei este produto, funciona perfeitamente!\n",
      "âœ… True Label: positive\n",
      "ðŸ”® Predicted: Positive (95.8%)\n",
      "------------------------------------------------------------\n",
      "ðŸ“ Text: O produto veio com defeito. Decepcionado.\n",
      "âœ… True Label: negative\n",
      "ðŸ”® Predicted: Negative (84.16%)\n",
      "------------------------------------------------------------\n",
      "ðŸ“ Text: Entrega super rÃ¡pida e produto Ã³timo.\n",
      "âœ… True Label: positive\n",
      "ðŸ”® Predicted: Positive (85.06%)\n",
      "------------------------------------------------------------\n",
      "ðŸ“ Text: NÃ£o recomendo, pÃ©ssima qualidade.\n",
      "âœ… True Label: negative\n",
      "ðŸ”® Predicted: Positive (75.45%)\n",
      "------------------------------------------------------------\n",
      "ðŸ“ Text: Excelente atendimento e produto maravilhoso.\n",
      "âœ… True Label: positive\n",
      "ðŸ”® Predicted: Positive (92.26%)\n",
      "------------------------------------------------------------\n",
      "ðŸ“ Text: DesperdÃ­cio de dinheiro. Arrependido.\n",
      "âœ… True Label: negative\n",
      "ðŸ”® Predicted: Positive (84.12%)\n",
      "------------------------------------------------------------\n",
      "ðŸ“ Text: Muito satisfeito com a minha compra!\n",
      "âœ… True Label: positive\n",
      "ðŸ”® Predicted: Positive (76.81%)\n",
      "------------------------------------------------------------\n",
      "ðŸ“ Text: O suporte ao cliente Ã© horrÃ­vel.\n",
      "âœ… True Label: negative\n",
      "ðŸ”® Predicted: Negative (62.13%)\n",
      "------------------------------------------------------------\n",
      "ðŸ“ Text: Compra perfeita, sem problemas.\n",
      "âœ… True Label: positive\n",
      "ðŸ”® Predicted: Negative (60.29%)\n",
      "------------------------------------------------------------\n",
      "ðŸ“ Text: Nunca mais compro nesta loja.\n",
      "âœ… True Label: negative\n",
      "ðŸ”® Predicted: Negative (67.74%)\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 2. Lightweight BERT Testing\n",
    "\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "\n",
    "# Paths\n",
    "checkpoint_path = \"./results_bert/checkpoint-9000\"  # Change to the right checkpoint if different\n",
    "model_name = \"google/bert_uncased_L-4_H-256_A-4\"\n",
    "json_file_path = f\"{base_path}/test_samples_multilang.json\"\n",
    "\n",
    "# Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint_path)\n",
    "\n",
    "# Create inference pipeline\n",
    "classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
    "label_map = {\n",
    "    \"LABEL_0\": \"Negative\",\n",
    "    \"LABEL_1\": \"Positive\"\n",
    "}\n",
    "\n",
    "# Load the JSON test samples\n",
    "with open(json_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    test_samples = json.load(f)\n",
    "\n",
    "# Perform inference and print results\n",
    "print(\"ðŸ” Running inference on test samples...\\n\")\n",
    "for sample in test_samples:\n",
    "    text = sample[\"text\"]\n",
    "    true_label = sample[\"expected_label\"]\n",
    "\n",
    "    prediction = classifier(text)[0]\n",
    "    predicted_label = label_map.get(prediction[\"label\"], prediction[\"label\"])\n",
    "    confidence = round(prediction[\"score\"] * 100, 2)\n",
    "\n",
    "    print(f\"ðŸ“ Text: {text}\")\n",
    "    print(f\"âœ… True Label: {true_label}\")\n",
    "    print(f\"ðŸ”® Predicted: {predicted_label} ({confidence}%)\\n{'-'*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6KIEUtjDtK_0"
   },
   "source": [
    "## 6. **Model Evaluation**\n",
    "\n",
    "This section compares the performance of two models trained for **multilingual sentiment classification**:\n",
    "\n",
    "- **LSTM (Long Short-Term Memory)**\n",
    "- **Lightweight BERT** (`google/bert_uncased_L-4_H-256_A-4`)\n",
    "\n",
    "---\n",
    "\n",
    "### **Evaluation Metrics**\n",
    "\n",
    "Both models were evaluated using standard classification metrics:\n",
    "\n",
    "- **Accuracy**: Overall correctness of the model.\n",
    "- **Precision**: Proportion of positive identifications that were correct.\n",
    "- **Recall**: Proportion of actual positives that were correctly identified.\n",
    "- **F1-Score**: Harmonic mean of precision and recall.\n",
    "- **Loss**: Binary cross-entropy loss used during training.\n",
    "\n",
    "---\n",
    "\n",
    "### **LSTM Model Evaluation**\n",
    "\n",
    "| Metric     | Value   |\n",
    "|------------|---------|\n",
    "| Accuracy   | 84.24%  |\n",
    "| F1-Score   | 84.42%  |\n",
    "| Precision  | 84%     |\n",
    "| Recall     | 84%     |\n",
    "| Model Type | Custom LSTM |\n",
    "| Device     | CPU     |\n",
    "\n",
    "---\n",
    "### ðŸ¤– **Lghtweight BERT Evaluation**\n",
    "\n",
    "| Metric     | Value    |\n",
    "|------------|----------|\n",
    "| Accuracy   | **92.01%** âœ… |\n",
    "| F1-Score   | **92.01%** âœ… |\n",
    "| Precision  | 92.01%   |\n",
    "| Recall     | 92.01%   |\n",
    "| Loss       | 0.1792   |\n",
    "| Model Type | Fine-tuned Transformer |\n",
    "| Device     | GPU (Google Colab) |\n",
    "\n",
    "\n",
    "### **Insights**\n",
    "\n",
    "- **Lightweight BERT** outperforms LSTM across all key metrics.\n",
    "- LSTM performs reasonably well and may be suitable for low-resource environments.\n",
    "- Pretrained transformer models like BERT provide strong contextual understanding, especially helpful in multilingual settings.\n",
    "\n",
    "---\n",
    "\n",
    "### **Conclusion**\n",
    "\n",
    "**Lightweight BERT is the recommended model** for production use due to its higher accuracy and robustness. LSTM serves as a good benchmark but falls short in comparison to transformer-based models, especially in multilingual sentiment classification."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
